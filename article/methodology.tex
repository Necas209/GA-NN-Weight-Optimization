A escolha da linguagem Python para este trabalho deve-se a várias vantagens que a mesma oferece.
Python é uma linguagem de programação de alto nível, reconhecida pela sua sintaxe simples e legibilidade, facilitando o desenvolvimento e a manutenção do código.
Por outro lado, Python possui uma ampla gama de bibliotecas e ‘frameworks’ para aprendizagem de máquina e computação científica, como NumPy, scikit-learn e PyTorch, que facilitam a implementação de redes neuronais e algoritmos genéticos.

De modo a poder comparar os resultados obtidos, o algoritmo genético foi implementado de raiz e com recurso à biblioteca PyGAD~\cite{Gad2021PyGAD:Library}.
PyGAD é uma biblioteca ‘open-source’, compatível com Keras e PyTorch, utilizada para construir algoritmos genéticos e otimizar algoritmos de ML. Este oferece diferentes tipos de operadores de cruzamento, mutação e seleção, permitindo a otimização de inúmeros problemas, através da personalização da função de aptidão.


\section{Modelo de Classificação}\label{sec:neural_net}

Tal como foi referido em~\ref{ch:problem}, e respeitando as características do dataset IRIS, a rede neuronal tem 4 neurónios na camada de entrada, 10 neurónios na camada oculta, e 3 neurónios na camada de saída, de modo a classificar as 3 espécies.

\begin{figure}[htbp]
    \centering
    \begin{neuralnetwork}[height=5]
        \newcommand{\x}[2]{$x_#2$}
        \newcommand{\y}[2]{$y_#2$}
        \newcommand{\h}[2]{\ifnum #2=4 $\cdots$ \else {\ifnum #2>4 $h_{10}$ \else $h_#2$ \fi} \fi}
        \newcommand{\hfirst}[2]{\small $h^{(1)}_#2$}
        \inputlayer[count=4, bias=false, title=Camada\\de entrada, text=\x]
        \hiddenlayer[count=5, bias=false, title=Camada oculta, text=\h] \linklayers
        \outputlayer[count=3, title=Camada\\de saída, text=\y] \linklayers
    \end{neuralnetwork}
    \caption{Estrutura do modelo de classificação}
    \label{fig:neural_net}
\end{figure}

O modelo em questão é uma \qq{Fully Connected Neural Network} (FCN), também conhecida por Rede Neuronal Totalmente Conectada, como se poder ver pela Fig.~\ref{fig:neural_net}.
Trata-se de uma arquitetura de rede neuronal artificial na qual cada neurónio numa camada está conectado a todos os neurónios da camada seguinte - conexões totalmente conectadas ou conexões densas.

\begin{table}[htpb]
    \centering
    \begin{tabular}{cccc}
        \hline
        Camada         & Pesos & Bias & Parâmetros \\ \hline
        Entrada (X)    & 40    & 10   & 50         \\
        Oculta (H)     & 30    & 3    & 33         \\
        Saída (Y)      & -     & -    & -          \\ \hline
        \textbf{Total} & 50    & 33   & 83         \\ \hline
    \end{tabular}
    \caption{Detalhes da rede neuronal}
    \label{tab:nn_summary}
\end{table}

Assim, e segundo a informação disponível na Tabela~\ref{tab:nn_summary}, o modelo em questão contém um total de 83 parâmetros que podem ser otimizados pelo algoritmo genético descrito na secção seguinte.


\section{Algoritmo Genético}\label{sec:gen_alg}

O algoritmo genético desenvolvido, intitulado \texttt{GeneticAlgorithm} e descrito em anexo~\ref{ch:appendix}, apresenta os seguintes atributos configuráveis:

\begin{table}[htbp]
    \centering
    \begin{tabular}{ccc} \hline
        \textbf{Nome} & \textbf{Tipo} & \textbf{Por omissão} \\ \hline
        \texttt{model} & \texttt{nn.Module} & - \\
        \texttt{population\_size} & \texttt{int} & 10 \\
        \texttt{mutation\_rate} & \texttt{float} & 0.05 \\
        \texttt{neuron\_off\_rate} & \texttt{float} & $10^{-3}$ \\
        \texttt{crossover\_rate} & \texttt{float} & 0.95 \\
        \texttt{elitism} & \texttt{bool} & \texttt{True} \\
        \texttt{num\_generations} & \texttt{int} & 100 \\
        \texttt{on\_generation\_interval} & \texttt{int} & 10 \\
        \texttt{best\_score} & \texttt{float} & 0.0 \\
        \texttt{best\_solution} & \texttt{np.ndarray} & \texttt{None} \\
        \texttt{fitness\_scores} & \texttt{list[float]} & - \\
        \texttt{fitness\_fn} & \texttt{Callable} & \texttt{None} \\
        \texttt{on\_generation} & \texttt{Callable} & \texttt{None} \\ \hline
    \end{tabular}
    \caption{Parâmetros do algoritmo genético}
    \label{tab:ga_params}
\end{table}

O método \texttt{run}, detalhado em Alg.~\ref{alg:ga_run}, é responsável pela execução do algoritmo genético por um determinado número de gerações, de acordo com os parâmetros fornecidos na instanciação do mesmo.

Inicialmente, a população de pesos é inicializada aleatoriamente de acordo com o tamanho desejado e o número de parâmetros do modelo a otimizar. De seguida, é executado o ciclo principal, que inclui as seguintes etapas:
\begin{itemize}
    \item Seleção dos pais: Os pais são selecionados a partir da população atual, usando o método \texttt{select\_parents};
    \item Cruzamento: A informação genética dos pais é combinada usando o operador de cruzamento definido no método \texttt{crossover};
    \item Mutação: A informação genética dos filhos sofre alterações usando o método \texttt{mutate}, podendo ser modificada ou anulada (desligar a conexão);
    \item Atualização da população: A população atual é substituída pelos filhos;
    \item Preservação do melhor indivíduo: O melhor indivíduo é preservado usando elitismo, substituindo o pior indivíduo na população;
    \item Armazenamento da aptidão: A aptidão do melhor indivíduo é armazenada na lista de melhores aptidões para posterior análise;
    \item Chamada da função \texttt{on\_generation}: Se a geração for múltipla do intervalo de gerações especificado, a função \texttt{on\_generation} é chamada para fornecer informações sobre o estado atual do algoritmo genético.
\end{itemize}

O operador de seleção, definido em Alg.~\ref{alg:select_parents}, utiliza um processo chamado seleção proporcional à aptidão (ou seleção por roleta) para escolher os pais da próxima geração, no qual é atribuída uma probabilidade de seleção a cada indivíduo com base na sua aptidão relativa. Quanto maior a aptidão de um indivíduo, maior a probabilidade de ele ser selecionado. Este processo é repetido até que pares suficientes de pais sejam selecionados para a reprodução.

\SetKwFunction{random}{random}
\SetKwFunction{selectParents}{selectParents}
\SetKwFunction{crossover}{crossover}
\SetKwFunction{mutate}{mutate}
\SetKwFunction{calculateScores}{calculateScores}
\SetKwFunction{max}{max}
\SetKwFunction{min}{min}
\SetKwFunction{onGeneration}{onGeneration}
\begin{algorithm}
    \caption{Execução do algoritmo genético (\texttt{run})}\label{alg:ga_run}
    $population \gets \random{populationSize, model.numParams}$\;
    \For{$generation \gets 0$ \KwTo $numGenerations$}{
        $parents \gets \selectParents{population}$\;
        $children \gets \mutate{\crossover{parents}}$\;
        $population \gets children$\;
        $scores \gets \calculateScores{population}$\;
        $(maxScore, maxScoreIdx) \gets \max{scores}$\;
        \If{$maxScore > bestScore$}{
            $bestScore \gets maxScore$\;
            $bestSolution \gets population[maxScoreIdx]$\;
        }
        \If{$elitism$}{
            $(\_, minScoreIdx) \gets \min{scores}$\;
            $population[minScoreIdx] \gets bestSolution$\;
        }
        $fitnessScores.add(bestScore)$\;
        \If{$generation$ $\%$ $onGenerationInterval == 0$}{
            $\onGeneration{generation, scores}$\;
        }
    }
\end{algorithm}

\begin{listing}[!ht]
\begin{minted}{python}
def select_parents(self, population: Population) -> Population:
    scores = self.calculate_scores(population)
    normalized_scores = scores / np.sum(scores)
    parent_indices = np.random.choice(
        range(self.population_size),
        size=(self.population_size // 2, 2),
        replace=True,
        p=normalized_scores
    )
    parents = population[parent_indices]
    return parents
\end{minted}
\caption{Operador de seleção}
\label{alg:select_parents}
\end{listing}

No caso do operador de cruzamento, definido em Alg.~\ref{alg:crossover}, é utilizada a técnica de cruzamento num único ponto, ou \qq{single point crossover}. Nessa técnica, um ponto de cruzamento é escolhido aleatoriamente e o material genético é trocado entre os pais até esse ponto, resultando nos filhos. A probabilidade de ocorrer cruzamento é determinada pelo valor de \texttt{crossover\_rate}. Esse processo é repetido para cada par de pais, gerando o par correspondente de filhos.

\begin{listing}[!ht]
\begin{minted}{python}
def crossover(self, parents1: Population, parents2: Population):
    sh = parents1.shape
    cross_pts = np.random.randint(1, sh[1], size=sh[0])
    mask = np.random.random(size=sh) < self.crossover_rate
    crossover_mask = np.arange(sh[1]) < cross_pts[:, np.newaxis]
    mask = np.logical_and(mask, crossover_mask)
    child1 = parents1 * np.logical_not(mask) + parents2 * mask
    child2 = parents2 * np.logical_not(mask) + parents1 * mask
    children = np.concatenate((child1, child2), axis=0)
    return children
\end{minted}
\caption{Operador de cruzamento}
\label{alg:crossover}
\end{listing}


Por fim, o operador de mutação, definido em Alg.~\ref{alg:mutate}, atua sobre a população de filhos, onde alguns neurónios são desativados (definidos como 0), e alguns parâmetros sofrem mutação adicionando valores aleatórios extraídos de uma distribuição normal com $\mu=0$ e $\sigma=0.1$. Isso introduz variabilidade e diversidade genética na população de filhos.

\begin{listing}[!ht]
\begin{minted}{python}
def mutate(self, children: Population) -> Population:
    mask_off = (np.random.random(size=children.shape) 
                < self.neuron_off_rate)
    mask_mutate = (np.random.random(size=children.shape) 
                   < self.mutation_rate)
    children[mask_off] = 0.0
    children[mask_mutate] += np.random.normal(scale=0.1, 
        size=np.sum(mask_mutate))
    return children
\end{minted}
\caption{Operador de mutação}
\label{alg:mutate}
\end{listing}


\section{Execução}\label{sec:execution}

